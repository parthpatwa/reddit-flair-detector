{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import The Required Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import datetime as dt\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> variable declaration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \"Scheduled\", \"Photography\", \"Science/Technology\", \"Politics\", \"Business/Finance\", \"Policy/Economy\", \"Sports\", \"Food\", \"AMA\"]\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_?]')\n",
    "#STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> utility functions </h3><br>\n",
    "clean_text function will remove unneccesary symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "def string_form(value):\n",
    "    return str(value)\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    #text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> scraping reddit data </h3><br>\n",
    "There are total 12 fliars. I collected only 100 posts per fliar as I think they should be sufficient for a POC, and due to lack of computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = \"0hliVZBlDhlipQ\",client_secret = \"pjUJDMMXhYaEmyJaLkQYb_2Fccg\",user_agent = \"Reddit Flair Detector\",username = \"chandan21121998\",password = \"Chandan@1234\")\n",
    "subreddit = reddit.subreddit('india')\n",
    "topics_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"comms_num\": [], \"created\": [], \"body\":[], \"author\":[], \"comments\":[]}\n",
    "\n",
    "for flair in flairs:\n",
    "    print(flair)\n",
    "    get_subreddits = subreddit.search(flair, limit=100)\n",
    "    #print('len', len(list(get_subreddits)))\n",
    "    for submission in get_subreddits:\n",
    "        topics_dict[\"flair\"].append(flair)\n",
    "        topics_dict[\"title\"].append(submission.title)\n",
    "        topics_dict[\"score\"].append(submission.score)\n",
    "        topics_dict[\"id\"].append(submission.id)\n",
    "        topics_dict[\"url\"].append(submission.url)\n",
    "        topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "        topics_dict[\"created\"].append(submission.created)\n",
    "        topics_dict[\"body\"].append(submission.selftext)\n",
    "        topics_dict[\"author\"].append(submission.author)\n",
    "    \n",
    "        submission.comments.replace_more(limit=None)\n",
    "        comment = ''\n",
    "        comment_c = 0\n",
    "        for top_level_comment in submission.comments:\n",
    "            comment = comment + ' ' + top_level_comment.body\n",
    "            comment_c += 1\n",
    "            if comment_c> 20:\n",
    "                break\n",
    "        topics_dict[\"comments\"].append(comment)\n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> preprocess and save the data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_timestamp = topics_data[\"created\"].apply(get_date)\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)\n",
    "topics_data.to_csv('reddit-india-data.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data = pd.read_csv('reddit-india-data.csv')\n",
    "topics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics_data['title'] = topics_data['title'].apply(string_form)\n",
    "topics_data['body'] = topics_data['body'].apply(string_form)\n",
    "topics_data['comments'] = topics_data['comments'].apply(string_form)\n",
    "\n",
    "topics_data['title'] = topics_data['title'].apply(clean_text)\n",
    "topics_data['body'] = topics_data['body'].apply(clean_text)\n",
    "topics_data['comments'] = topics_data['comments'].apply(clean_text)\n",
    "topics_data.to_csv('reddit-india-data.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
