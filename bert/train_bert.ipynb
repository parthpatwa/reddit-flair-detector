{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import os\n",
    "import torch.utils.data\n",
    "sys.path.append('../')\n",
    "#import experiments.config as C\n",
    "from transformers import *\n",
    "\n",
    "\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "import shutil\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(7)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "random.seed(3)\n",
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BaseAttention(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super(BaseAttention, self).__init__()\n",
    "\n",
    "        self.u = nn.Linear(dimension, dimension)\n",
    "        self.v = nn.Parameter(torch.rand(dimension), requires_grad=True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.epsilon = 1e-10\n",
    "\n",
    "    def forward(self, h, mask):\n",
    "\n",
    "        u_it = self.tanh(self.u(h))\n",
    "        alpha = torch.exp(torch.matmul(u_it, self.v))\n",
    "        alpha = mask * alpha + self.epsilon\n",
    "        denominator_sum = torch.sum(alpha, dim=-1, keepdim=True)\n",
    "        alpha = mask * (alpha / denominator_sum)\n",
    "\n",
    "        output = h * alpha.unsqueeze(2)\n",
    "        output = torch.sum(output, dim=1)\n",
    "\n",
    "        return output, alpha\n",
    "    \n",
    "class BERTRA(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERTRA, self).__init__()\n",
    "\n",
    "        self.embedding_dim = 768\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True,\n",
    "                                                                output_attentions=True)\n",
    "\n",
    "        self.attention = BaseAttention(self.embedding_dim)\n",
    "\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 500),\n",
    "\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(500 , 100),\n",
    "\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(100, 12) \n",
    "\n",
    "\n",
    "    def forward(self, sentences, mask):\n",
    "\n",
    "        hidden, _ = self.bert(sentences)[-2:]\n",
    "        sentences = hidden[-1]\n",
    "\n",
    "        attention_applied, attention_weights = self.attention(sentences, mask.float())\n",
    "\n",
    "        x = self.sequential(attention_applied)\n",
    "        out = F.softmax(self.output(x), -1)\n",
    "\n",
    "\n",
    "        return {\n",
    "            'y_pred': out,\n",
    "            'weights': attention_weights\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['TorchHelper']\n",
    "\n",
    "\n",
    "class TorchHelper:\n",
    "    checkpoint_history = []\n",
    "    early_stop_monitor_vals = []\n",
    "    best_score = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "    def show_progress(self, current_iter, total_iter, start_time, training_loss, additional_msg=''):\n",
    "        bar_length = 50\n",
    "        ratio = current_iter / total_iter\n",
    "        progress_length = int(ratio * bar_length)\n",
    "        percents = int(ratio * 100)\n",
    "        bar = '[' + '=' * (progress_length - 1) + '>' + '-' * (bar_length - progress_length) + ']'\n",
    "\n",
    "        current_time = time.time()\n",
    "        # elapsed_time = time.gmtime(current_time - start_time).tm_sec\n",
    "        elapsed_time = round(current_time - start_time, 0)\n",
    "        estimated_time_needed = round((elapsed_time / current_iter) * (total_iter - current_iter), 0)\n",
    "\n",
    "# sys.stdout.write\n",
    "        print(\n",
    "            'Iter {}/{}: {} {}%  Loss: {} ETA: {}s, Elapsed: {}s, TLI: {} {} '.format(current_iter, total_iter, bar,\n",
    "                                                                                       percents,\n",
    "                                                                                       round(training_loss, 4),\n",
    "                                                                                       estimated_time_needed,\n",
    "                                                                                       elapsed_time,\n",
    "                                                                                       np.round(\n",
    "                                                                                           elapsed_time / current_iter,\n",
    "                                                                                           3), additional_msg), end = \"\\r\")\n",
    "\n",
    "        if current_iter < total_iter:\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write('\\n')\n",
    "\n",
    "    def checkpoint_model(self, model_to_save, optimizer_to_save, path_to_save, current_score, epoch, mode='max'):\n",
    "        \"\"\"\n",
    "        Checkpoints models state after each epoch.\n",
    "        :param model_to_save:\n",
    "        :param optimizer_to_save:\n",
    "        :param path_to_save:\n",
    "        :param current_score:\n",
    "        :param epoch:\n",
    "        :param n_epoch:\n",
    "        :param mode:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        model_state = {'epoch'      : epoch + 1,\n",
    "                       'model_state': model_to_save.state_dict(),\n",
    "                       'score'      : current_score,\n",
    "                       'optimizer'  : optimizer_to_save.state_dict()}\n",
    "\n",
    "        # Save the model as a regular checkpoint\n",
    "        torch.save(model_state, path_to_save + 'last.pth'.format(epoch))\n",
    "\n",
    "        self.checkpoint_history.append(current_score)\n",
    "        is_best = False\n",
    "\n",
    "        # If the model is best so far according to the score, save as the best model state\n",
    "        if ((np.max(self.checkpoint_history) == current_score and mode == 'max') or\n",
    "                (np.min(self.checkpoint_history) == current_score and mode == 'min')):\n",
    "            is_best = True\n",
    "            self.best_score = current_score\n",
    "            self.best_epoch = epoch\n",
    "            # print('inside checkpoint', current_score, np.max(self.checkpoint_history))\n",
    "            # torch.save(model_state, path_to_save + '{}_best.pth'.format(n_epoch))\n",
    "            torch.save(model_state, path_to_save + 'best.pth')\n",
    "            print('BEST saved')\n",
    "\n",
    "        print('Current best', round(max(self.checkpoint_history), 4), 'after epoch {}'.format(self.best_epoch))\n",
    "\n",
    "        return is_best\n",
    "\n",
    "\n",
    "    def load_saved_model(self, model, path):\n",
    "        \"\"\"\n",
    "        Load a saved model from dump\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # self.active_model.load_state_dict(self.best_model_path)['model_state']\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_helper = TorchHelper()\n",
    "start_epoch = 0\n",
    "batch_size = 4\n",
    "max_epochs = 30\n",
    "learning_rate = 0.00001\n",
    "optimizer_type = 'adam'\n",
    "l2_regularize = True\n",
    "l2_lambda = 0.01\n",
    "\n",
    "\n",
    "alphabet_path = \"alphabet.json\"\n",
    "\n",
    "# Creates the directory where the results, logs, and models will be dumped.\n",
    "\n",
    "run_name = 'bert_base_attn_adam_lr1e5'\n",
    "\n",
    "description = ''\n",
    "\n",
    "\n",
    "#output_dir_path = '../results/' + run_name + '/'\n",
    "#if not os.path.exists(output_dir_path):\n",
    "#    os.mkdir(output_dir_path)\n",
    "\n",
    "run_mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loaded\n",
      "Validation Loaded\n",
      "Data Split: Train (876), Dev (224)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Load Data\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "features_train = json.load(open('../train.json'))\n",
    "features_dev = json.load(open('../test.json'))\n",
    "\n",
    "train_set = [val for key,val in features_train.items()]\n",
    "print('Train Loaded')\n",
    "\n",
    "validation_set = [val for key,val in features_dev.items()]\n",
    "print('Validation Loaded')\n",
    "\n",
    "# train_set = train_set[:100]\n",
    "# validation_set = validation_set[:100]\n",
    "\n",
    "print('Data Split: Train (%d), Dev (%d)' % (len(train_set), len(validation_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Functions\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Creates and returns the model.\n",
    "    Moves to GPU if found any.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    model = BERTRA()\n",
    "\n",
    "    model.cuda()\n",
    "    if run_mode == 'resume':\n",
    "        torch_helper.load_saved_model(model, output_dir_path + 'best.pth')\n",
    "        print('model loaded')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def compute_l2_reg_val(model):\n",
    "    if not l2_regularize:\n",
    "        return 0.\n",
    "\n",
    "    l2_reg = None\n",
    "\n",
    "    for w in model.parameters():\n",
    "        if l2_reg is None:\n",
    "            l2_reg = w.norm(2)\n",
    "        else:\n",
    "            l2_reg = l2_reg + w.norm(2)\n",
    "\n",
    "    return l2_lambda * l2_reg.item()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Padding\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def pad_features(docs_ints, seq_length=700):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(docs_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(docs_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    return features\n",
    "\n",
    "def masking(docs_ints, seq_length=700):\n",
    "\n",
    "    # getting the correct rows x cols shape\n",
    "    masks = np.zeros((len(docs_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and\n",
    "    for i, row in enumerate(docs_ints):\n",
    "        #mask[i, :len(row)] = 1\n",
    "        masks[i, -len(row):] = 1\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def train(model, optimizer, shuffled_train_set):\n",
    "    \"\"\"\n",
    "    Trains the model using the optimizer for a single epoch.\n",
    "    :param model: pytorch model\n",
    "    :param optimizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    batch_x,batch_y, = [], []\n",
    "    random.Random(1234).shuffle(shuffled_train_set)\n",
    "\n",
    "    for i in range(len(shuffled_train_set)):\n",
    "\n",
    "        batch_x.append(shuffled_train_set[i]['tokenized'])\n",
    "\n",
    "        batch_y.append(shuffled_train_set[i]['y'])\n",
    "\n",
    "        if len(batch_x) == batch_size or i == len(shuffled_train_set) - 1:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mask = masking(batch_x)\n",
    "            padded = pad_features(batch_x)\n",
    "\n",
    "\n",
    "            out = model(torch.tensor(padded,dtype=torch.long).cuda(), torch.tensor(mask,dtype=torch.long).cuda())\n",
    "\n",
    "            y_pred = out['y_pred'].cpu()\n",
    "            loss = F.cross_entropy(y_pred, torch.max(torch.Tensor(batch_y), 1)[1]) + compute_l2_reg_val(model)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            torch_helper.show_progress(batch_idx , np.ceil(len(shuffled_train_set) / batch_size), start_time,\n",
    "                                   round(total_loss / (i+1), 4))\n",
    "            batch_idx += 1\n",
    "            batch_x,  batch_y = [], []\n",
    "            \n",
    "    return model, shuffled_train_set\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Evaluate the model\n",
    "# ----------------------------------------------------------------------------\n",
    "def evaluate(model, dev_set):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    batch_x, batch_y = [], []\n",
    "    y_true = []\n",
    "\n",
    "    label_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dev_set)):\n",
    "\n",
    "            batch_x.append(dev_set[i]['tokenized'])\n",
    "\n",
    "            batch_y.append(dev_set[i]['y'])\n",
    "\n",
    "\n",
    "            y_true.append(label2idx[dev_set[i]['label']])\n",
    "\n",
    "            if len(batch_x) == batch_size or i == len(dev_set) - 1:\n",
    "                mask = masking(batch_x)\n",
    "                padded = pad_features(batch_x)\n",
    "\n",
    "\n",
    "\n",
    "                # out = model(torch.tensor(padded).cuda(), batch_emoj.cuda(), torch.tensor(mask).cuda())\n",
    "                out = model(torch.tensor(padded,dtype=torch.long).cuda(), torch.tensor(mask,dtype=torch.long).cuda())\n",
    "\n",
    "                y_pred = out['y_pred'].cpu()\n",
    "\n",
    "                label_predictions.extend(list(torch.argmax(y_pred, -1).numpy()))\n",
    "\n",
    "                loss = F.cross_entropy(y_pred, torch.max(torch.Tensor(batch_y), 1)[1]) \n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                batch_x, batch_y, = [], []\n",
    "\n",
    "    weighted_f1 = f1_score(y_true, label_predictions, average='weighted')\n",
    "\n",
    "    return label_predictions, \\\n",
    "           total_loss/len(dev_set), \\\n",
    "           weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def training_loop():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model = create_model()\n",
    "\n",
    "\n",
    "    if optimizer_type == 'adamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.0)\n",
    "    elif optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    shuffled_train_set = train_set\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "\n",
    "        # for p in model.tm.parameters():\n",
    "        #     p.requires_grad = False\n",
    "\n",
    "        for p in model.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "        print('[Epoch %d] / %d : %s' % (epoch + 1, max_epochs, run_name))\n",
    "\n",
    "        model, shuffled_train_set = train(model, optimizer, shuffled_train_set)\n",
    "\n",
    "        #print(\"Training Done!\")\n",
    "\n",
    "        val_label_pred,  val_loss, val_label_f1 = evaluate(model, validation_set)\n",
    "        train_label_pred,  train_loss, train_label_f1 = evaluate(model, train_set)\n",
    "\n",
    "        #print('Evaluation Done!')\n",
    "\n",
    "        current_lr = 0\n",
    "        for pg in optimizer.param_groups:\n",
    "            current_lr = pg['lr']\n",
    "\n",
    "        print('Training Loss %.5f, Validation Loss %.5f' % (train_loss, val_loss))\n",
    "        print('Training Label weighted F1 %.5f, Validation Label weighted F1 %.5f' % (train_label_f1, val_label_f1))\n",
    "        # print('Learning Rate', current_lr)\n",
    "\n",
    "\n",
    "        is_best = torch_helper.checkpoint_model(model, optimizer, '..', val_label_f1, epoch + 1,\n",
    "                                                'max')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {'AskIndia': 0,\n",
    " 'Non-Political': 1,\n",
    " '[R]eddiquette': 2,\n",
    " 'Scheduled': 3,\n",
    " 'Photography': 4,\n",
    " 'Science/Technology': 5,\n",
    " 'Politics': 6,\n",
    " 'Business/Finance': 7,\n",
    " 'Policy/Economy': 8,\n",
    " 'Sports': 9,\n",
    " 'Food': 10,\n",
    " 'AMA': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.762 ETA: 0.0s, Elapsed: 81.0s, TLI: 0.37    \n",
      "Training Loss 0.62012, Validation Loss 0.61971\n",
      "Training Label weighted F1 0.09578, Validation Label weighted F1 0.07330\n",
      "BEST saved\n",
      "Current best 0.0733 after epoch 1\n",
      "[Epoch 2] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7617 ETA: 0.0s, Elapsed: 85.0s, TLI: 0.388  \n",
      "Training Loss 0.61916, Validation Loss 0.61900\n",
      "Training Label weighted F1 0.09519, Validation Label weighted F1 0.07259\n",
      "Current best 0.0733 after epoch 1\n",
      "[Epoch 3] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7614 ETA: 0.0s, Elapsed: 89.0s, TLI: 0.406  \n",
      "Training Loss 0.61877, Validation Loss 0.61882\n",
      "Training Label weighted F1 0.11422, Validation Label weighted F1 0.12775\n",
      "BEST saved\n",
      "Current best 0.1278 after epoch 3\n",
      "[Epoch 4] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7605 ETA: 0.0s, Elapsed: 89.0s, TLI: 0.406  \n",
      "Training Loss 0.61809, Validation Loss 0.61861\n",
      "Training Label weighted F1 0.09443, Validation Label weighted F1 0.08945\n",
      "Current best 0.1278 after epoch 3\n",
      "[Epoch 5] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7603 ETA: 0.0s, Elapsed: 93.0s, TLI: 0.425  \n",
      "Training Loss 0.61773, Validation Loss 0.61819\n",
      "Training Label weighted F1 0.10040, Validation Label weighted F1 0.13204\n",
      "BEST saved\n",
      "Current best 0.132 after epoch 5\n",
      "[Epoch 6] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7592 ETA: 0.0s, Elapsed: 93.0s, TLI: 0.425  \n",
      "Training Loss 0.61694, Validation Loss 0.61704\n",
      "Training Label weighted F1 0.08709, Validation Label weighted F1 0.11255\n",
      "Current best 0.132 after epoch 5\n",
      "[Epoch 7] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7593 ETA: 0.0s, Elapsed: 95.0s, TLI: 0.434  \n",
      "Training Loss 0.61712, Validation Loss 0.61749\n",
      "Training Label weighted F1 0.12284, Validation Label weighted F1 0.13803\n",
      "BEST saved\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 8] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7589 ETA: 0.0s, Elapsed: 94.0s, TLI: 0.429  \n",
      "Training Loss 0.61628, Validation Loss 0.61655\n",
      "Training Label weighted F1 0.10514, Validation Label weighted F1 0.12072\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 9] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7591 ETA: 0.0s, Elapsed: 95.0s, TLI: 0.434  \n",
      "Training Loss 0.61608, Validation Loss 0.61601\n",
      "Training Label weighted F1 0.09555, Validation Label weighted F1 0.13565\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 10] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7589 ETA: 0.0s, Elapsed: 96.0s, TLI: 0.438  \n",
      "Training Loss 0.61556, Validation Loss 0.61541\n",
      "Training Label weighted F1 0.10210, Validation Label weighted F1 0.11358\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 11] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7594 ETA: 0.0s, Elapsed: 96.0s, TLI: 0.438  \n",
      "Training Loss 0.61539, Validation Loss 0.61545\n",
      "Training Label weighted F1 0.11397, Validation Label weighted F1 0.11919\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 12] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7586 ETA: 0.0s, Elapsed: 95.0s, TLI: 0.434  \n",
      "Training Loss 0.61499, Validation Loss 0.61499\n",
      "Training Label weighted F1 0.10766, Validation Label weighted F1 0.11819\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 13] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.758 ETA: 0.0s, Elapsed: 93.0s, TLI: 0.425   \n",
      "Training Loss 0.61495, Validation Loss 0.61501\n",
      "Training Label weighted F1 0.11849, Validation Label weighted F1 0.13049\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 14] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7579 ETA: 0.0s, Elapsed: 92.0s, TLI: 0.42   \n",
      "Training Loss 0.61349, Validation Loss 0.61357\n",
      "Training Label weighted F1 0.10528, Validation Label weighted F1 0.10354\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 15] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7571 ETA: 0.0s, Elapsed: 95.0s, TLI: 0.434  \n",
      "Training Loss 0.61353, Validation Loss 0.61335\n",
      "Training Label weighted F1 0.11155, Validation Label weighted F1 0.11514\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 16] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7582 ETA: 0.0s, Elapsed: 92.0s, TLI: 0.42   \n",
      "Training Loss 0.61330, Validation Loss 0.61328\n",
      "Training Label weighted F1 0.11479, Validation Label weighted F1 0.10907\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 17] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.756 ETA: 0.0s, Elapsed: 99.0s, TLI: 0.452   \n",
      "Training Loss 0.61289, Validation Loss 0.61268\n",
      "Training Label weighted F1 0.12224, Validation Label weighted F1 0.12517\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 18] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7572 ETA: 0.0s, Elapsed: 93.0s, TLI: 0.425  \n",
      "Training Loss 0.61278, Validation Loss 0.61253\n",
      "Training Label weighted F1 0.13563, Validation Label weighted F1 0.11934\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 19] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7559 ETA: 0.0s, Elapsed: 98.0s, TLI: 0.447  \n",
      "Training Loss 0.61188, Validation Loss 0.61201\n",
      "Training Label weighted F1 0.13588, Validation Label weighted F1 0.13798\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 20] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7553 ETA: 0.0s, Elapsed: 96.0s, TLI: 0.438  \n",
      "Training Loss 0.61238, Validation Loss 0.61241\n",
      "Training Label weighted F1 0.14651, Validation Label weighted F1 0.11233\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 21] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7555 ETA: 0.0s, Elapsed: 98.0s, TLI: 0.447  \n",
      "Training Loss 0.61277, Validation Loss 0.61305\n",
      "Training Label weighted F1 0.13443, Validation Label weighted F1 0.12824\n",
      "Current best 0.138 after epoch 7\n",
      "[Epoch 22] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.756 ETA: 0.0s, Elapsed: 97.0s, TLI: 0.443   \n",
      "Training Loss 0.61115, Validation Loss 0.61114\n",
      "Training Label weighted F1 0.13844, Validation Label weighted F1 0.14248\n",
      "BEST saved\n",
      "Current best 0.1425 after epoch 22\n",
      "[Epoch 23] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7567 ETA: 0.0s, Elapsed: 94.0s, TLI: 0.429  \n",
      "Training Loss 0.61118, Validation Loss 0.61134\n",
      "Training Label weighted F1 0.13725, Validation Label weighted F1 0.11993\n",
      "Current best 0.1425 after epoch 22\n",
      "[Epoch 24] / 30 : bert_base_attn_adam_lr1e5\n",
      "Iter 219/219.0: [=================================================>] 100%  Loss: 10.7544 ETA: 0.0s, Elapsed: 94.0s, TLI: 0.429  \n"
     ]
    }
   ],
   "source": [
    "model = training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
